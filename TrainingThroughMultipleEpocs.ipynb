{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.autograd.grad_mode.set_grad_enabled at 0x11a002c88>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds,labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data/FashionMNIST',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "    \n",
    "    def forward(self, t):\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t) \n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        t = self.out(t)\n",
    "\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "loss1: 2.3079214096069336\nloss1: 2.2951629161834717\n"
    }
   ],
   "source": [
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01) #Network Weights are in here\n",
    "\n",
    "batch = next(iter(train_loader)) # Get Batch\n",
    "images, labels = batch\n",
    "\n",
    "preds = network(images) # Pass Batch\n",
    "loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "loss.backward() # Calculate Gradients\n",
    "optimizer.step() # Update Weights\n",
    "\n",
    "print('loss1:', loss.item())\n",
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds, labels)\n",
    "print('loss1:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "epoch: 0 total_correct: 47212 loss: 334.7672748565674\n"
    }
   ],
   "source": [
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01) #Network Weights are in here\n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "\n",
    "# batch = next(iter(train_loader)) # Get Batch\n",
    "for batch in train_loader:\n",
    "    images, labels = batch\n",
    "\n",
    "    preds = network(images) # Pass Batch\n",
    "    loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "    optimizer.zero_grad() # PyTorch Adds to this, so you have to zero it out after one batch\n",
    "    loss.backward() # Calculate Gradients\n",
    "    optimizer.step() # Update Weights\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "print(\"epoch:\", 0, \"total_correct:\", total_correct, \"loss:\", total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.7868666666666667"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_correct / len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "epoch: 0 total_correct: 46180 loss: 364.37743493914604\nepoch: 1 total_correct: 50833 loss: 246.43286822736263\nepoch: 2 total_correct: 51676 loss: 223.522591650486\nepoch: 3 total_correct: 52130 loss: 212.12095564603806\nepoch: 4 total_correct: 52296 loss: 206.46559286117554\n"
    }
   ],
   "source": [
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01) #Network Weights are in here\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    # batch = next(iter(train_loader)) # Get Batch\n",
    "    for batch in train_loader:\n",
    "        images, labels = batch\n",
    "\n",
    "        preds = network(images) # Pass Batch\n",
    "        loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "        optimizer.zero_grad() # PyTorch Adds to this, so you have to zero it out after one batch\n",
    "        loss.backward() # Calculate Gradients\n",
    "        optimizer.step() # Update Weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "    print(\"epoch:\", epoch, \"total_correct:\", total_correct, \"loss:\", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8716"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_correct / len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "e-01],\n          [-4.7825e-01, -3.8159e-01, -4.4542e-01, -2.5959e-01, -4.0990e-01],\n          [-3.9337e-01, -4.0768e-01, -3.9919e-01, -3.0776e-01, -3.7316e-01],\n          [-5.1077e-01, -4.6276e-01, -3.7039e-01, -2.3538e-01, -3.3991e-01],\n          [-3.5820e-01, -3.6214e-01, -4.8788e-01, -3.4681e-01, -4.3302e-01]],\n\n         [[ 1.5422e-01, -4.1945e-01, -3.6128e-01, -6.0054e-02, -1.6455e-02],\n          [ 1.3351e-01, -4.8144e-01, -1.6687e-01, -1.4987e-01,  1.3087e-01],\n          [-2.0308e-01, -3.0870e-01, -5.2527e-01, -1.3949e-01,  1.6658e-01],\n          [-1.6298e-01, -3.3641e-01, -2.0040e-01, -4.5271e-02,  1.3012e-01],\n          [-5.0621e-01, -2.6261e-01, -2.8340e-01, -2.6641e-02,  3.2039e-01]],\n\n         [[-4.2162e-01, -2.4276e-01, -5.2980e-01, -4.0144e-01, -2.6757e-01],\n          [-7.9771e-02, -1.4449e-01, -2.5188e-01, -2.7640e-01, -3.0085e-01],\n          [-1.1547e-01, -1.4793e-01, -5.3237e-01, -3.4014e-01, -1.1607e-01],\n          [-1.2901e-01, -4.1581e-01, -2.3483e-01, -3.3300e-01, -3.9045e-01],\n          [-2.5281e-01, -1.9664e-01, -3.4858e-01, -4.3877e-01, -2.2279e-01]]],\n\n\n        ...,\n\n\n        [[[ 2.7353e-01,  2.5338e-01,  2.1896e-02, -4.1844e-01, -1.2893e-01],\n          [ 1.4161e-01, -3.2137e-01, -5.3210e-01, -1.5941e-01, -5.3624e-02],\n          [-9.4377e-02, -4.6992e-01, -2.9765e-01, -1.1769e-01,  1.5883e-01],\n          [ 1.2163e-01,  5.3507e-02,  1.8703e-01, -7.8601e-02,  1.3943e-01],\n          [-2.5084e-01,  1.3710e-01,  2.2785e-01,  7.2389e-01,  1.2017e+00]],\n\n         [[-1.8023e-01, -3.5895e-01, -7.3526e-02, -5.3885e-01, -2.5270e-01],\n          [-2.5194e-01, -6.4796e-01, -1.3510e-01, -3.8549e-01,  1.1438e-01],\n          [ 7.6544e-02, -6.0499e-02,  3.7241e-01, -8.2476e-02,  8.1110e-04],\n          [ 2.2076e-01,  4.4058e-01, -3.9988e-02,  3.5250e-01, -8.1724e-02],\n          [-8.8710e-02,  5.1417e-02,  8.7837e-01,  7.7797e-01, -5.0365e-01]],\n\n         [[-2.8930e-01,  2.8378e-01,  2.0510e-01, -1.0634e-01, -6.5451e-02],\n          [-1.5819e-01, -4.7857e-02,  1.2433e-01,  1.7426e-01,  3.5939e-01],\n          [ 3.4693e-02,  2.4564e-01,  2.6956e-01, -1.3613e-01,  2.4971e-01],\n          [ 1.1254e-01,  3.0404e-01,  3.4578e-01, -1.4789e-01,  7.4526e-02],\n          [-3.1761e-01, -5.4154e-02,  1.1669e-01, -7.0291e-01, -5.6025e-02]],\n\n         [[-1.6876e-01,  1.2511e-01,  1.2772e-02,  5.3227e-02,  5.9597e-02],\n          [ 1.4442e-01,  2.5006e-01, -8.2346e-02, -4.5819e-02,  1.7391e-01],\n          [ 1.5199e-01,  7.5651e-02, -4.6637e-02, -1.7093e-01,  2.3579e-01],\n          [-5.4225e-02, -1.7726e-01, -8.0510e-02, -2.3209e-01,  1.1560e-01],\n          [-4.5973e-01, -4.5345e-01, -2.1160e-01, -8.4578e-02,  3.2934e-01]],\n\n         [[-1.0154e+00,  7.4867e-04,  1.8446e-01,  1.1395e-01,  1.5780e-01],\n          [-1.3562e+00, -2.1066e-01,  7.5892e-02, -2.1557e-01,  1.7273e-01],\n          [-9.0608e-01, -4.1601e-01,  2.7314e-01, -4.1317e-02, -1.5489e-01],\n          [-3.8220e-03, -2.8973e-01,  2.5279e-01, -1.4962e-01, -9.3515e-02],\n          [ 4.9979e-01, -1.4006e-01,  2.6109e-01, -8.4513e-01,  1.9335e-01]],\n\n         [[-7.0680e-03, -1.1276e-01, -4.9703e-01,  2.5276e-01, -4.4331e-01],\n          [ 4.1235e-01, -1.8070e-01, -7.5935e-01, -5.3645e-01, -3.6914e-01],\n          [ 2.9086e-01, -4.4962e-01, -8.0954e-01, -5.7190e-01, -1.2544e-01],\n          [-1.2326e-01, -1.7739e-01, -6.4058e-01, -1.7693e-01,  1.9025e-01],\n          [ 3.7386e-01,  3.9543e-01, -1.9232e-01,  2.2457e-01, -6.4810e-02]]],\n\n\n        [[[-3.4823e-02, -3.0966e-01, -1.4317e-01,  1.1729e-01, -1.7699e-01],\n          [ 9.8861e-02, -2.2071e-01,  2.9940e-01, -4.3721e-01, -2.4273e-02],\n          [-2.6499e-01, -2.4097e-01,  3.3983e-01, -1.1388e-01, -4.6650e-01],\n          [ 2.5925e-02, -4.0874e-01,  1.1424e-01,  3.3319e-01, -5.5892e-01],\n          [-3.1633e-02, -9.4247e-02, -2.0919e-01,  2.6832e-01,  1.6138e-01]],\n\n         [[-5.9093e-01,  4.4325e-01, -9.5764e-02,  8.4043e-01, -2.4873e-01],\n          [ 5.7130e-01,  9.0891e-01,  7.2798e-01,  1.0183e-01, -1.5639e-01],\n          [-1.2907e-01, -1.6518e-01, -3.8711e-01, -5.0665e-02, -8.3328e-01],\n          [-1.6438e-01,  1.4074e-02, -4.3255e-01, -4.0160e-01, -8.5378e-01],\n          [-2.2602e-01,  8.1961e-01, -1.2852e-01, -5.7075e-01, -9.5333e-01]],\n\n         [[-6.5999e-01,  1.4614e-01,  7.3693e-02,  3.3010e-01, -2.6202e-02],\n          [-4.2480e-01,  1.3941e-01,  1.5694e-01,  1.8340e-01, -9.5358e-02],\n          [-5.7941e-02, -4.8855e-01, -4.6886e-01, -2.0111e-01, -1.9132e-01],\n          [ 1.3879e-01,  3.9198e-02, -3.5517e-01, -7.0262e-02, -1.9774e-01],\n          [ 6.8469e-02,  3.8792e-01, -4.4919e-01, -1.3705e-02, -1.1270e-01]],\n\n         [[ 1.6129e-01,  7.3564e-02, -1.4976e-01, -1.4604e-01, -2.9121e-01],\n          [-6.3073e-02,  5.8706e-02, -3.2356e-02, -2.2032e-01, -3.0615e-01],\n          [-3.4197e-01,  1.1121e-01,  5.1551e-02, -1.5703e-01, -4.1969e-01],\n          [-2.3155e-01, -1.7312e-01,  1.0418e-01,  5.1897e-03, -6.1322e-01],\n          [-1.7293e-01, -2.5006e-01,  1.3522e-01,  3.3669e-02, -2.3397e-01]],\n\n         [[-6.2282e-01, -8.4724e-01, -2.9932e-01,  6.6600e-01,  5.2919e-01],\n          [-2.9991e-01, -4.3315e-01,  5.9178e-02,  1.3896e-01,  5.5946e-01],\n          [-5.5212e-01,  1.6716e-01, -6.6372e-01, -1.8846e-01,  1.2248e-01],\n          [ 1.1746e-01,  4.6869e-01, -8.6132e-01, -1.6908e-01,  8.6793e-02],\n          [-9.5297e-02,  8.1704e-01, -2.4556e-01,  1.1839e-01,  4.5480e-01]],\n\n         [[ 4.3524e-02,  4.5025e-03, -3.8357e-01, -1.3990e+00, -3.2605e-02],\n          [-4.5467e-01, -2.7434e-02, -2.6010e-01,  1.4349e-01,  4.0003e-01],\n          [-3.5986e-01, -4.5762e-01,  9.4195e-02,  1.6543e-01,  2.8532e-01],\n          [ 1.7361e-01, -1.2556e-01,  2.7404e-01, -1.1506e-01,  2.1477e-01],\n          [-9.5145e-02, -5.0668e-01, -1.8906e-01, -2.0097e-01,  4.4616e-01]]],\n\n\n        [[[-1.1308e-02,  2.6163e-01, -4.4308e-02, -1.4430e-01,  4.8417e-01],\n          [ 1.7166e-02,  2.4485e-01, -2.9190e-02, -5.6710e-01,  1.6632e-01],\n          [ 2.8921e-01,  4.1622e-02,  5.3241e-02,  2.3147e-01, -2.4232e-01],\n          [ 1.5614e-01,  3.3425e-01, -1.1483e-01,  4.8145e-01, -5.5279e-01],\n          [ 2.7312e-01, -2.3082e-01, -3.0815e-01,  1.0223e-01, -1.1984e-01]],\n\n         [[-5.8325e-01, -3.4911e-01,  7.8711e-01, -1.6860e-02,  4.8105e-01],\n          [-7.0558e-02, -1.2191e-01,  5.3127e-01,  3.2337e-02,  1.2153e-01],\n          [-2.4304e-01, -4.6753e-01,  5.6365e-02,  2.3237e-01,  8.1448e-02],\n          [-2.3189e-02, -8.0082e-02, -1.5506e-01,  2.0001e-01, -2.2729e-04],\n          [ 1.4585e-01,  5.0265e-03,  4.1816e-01, -1.1290e-01,  9.6591e-02]],\n\n         [[ 1.9849e-01, -1.2103e-01, -2.5174e-01, -2.5881e-01, -1.1763e-01],\n          [ 5.2029e-02,  2.2591e-01,  3.3016e-02, -1.4313e-01, -1.4263e-01],\n          [ 1.1158e-01, -3.4160e-02,  3.4214e-01,  1.7504e-01, -6.3909e-01],\n          [-9.0792e-02,  6.5579e-02,  2.4442e-01,  2.1848e-02, -3.6768e-01],\n          [ 2.2572e-01,  2.4045e-01, -6.0344e-02,  1.5958e-01,  8.4580e-02]],\n\n         [[ 1.3646e-01,  6.4622e-03, -2.9376e-01, -1.0262e-01,  1.9007e-01],\n          [ 4.1714e-01,  1.3123e-01, -2.2243e-01, -3.6418e-01,  2.3413e-02],\n          [ 1.5208e-01,  1.0914e-01, -2.9382e-02, -1.6438e-01, -7.1857e-02],\n          [ 3.4949e-02,  1.0197e-01,  1.8933e-01, -7.5173e-02, -9.7583e-02],\n          [ 3.1869e-01, -2.8103e-01, -3.0413e-02, -4.5932e-01, -2.8810e-01]],\n\n         [[ 4.6316e-01,  5.8999e-01, -2.3039e-01, -6.9328e-01, -5.7189e-01],\n          [ 3.2360e-01, -5.0650e-01,  3.4913e-01,  2.4174e-02, -3.0764e-01],\n          [ 4.8254e-02,  2.6028e-01,  2.4727e-01,  8.6642e-01,  5.4903e-02],\n          [-4.5559e-01, -4.0047e-01, -5.5196e-02,  5.9568e-01,  2.5890e-01],\n          [ 1.6467e-01, -1.4533e-01, -5.3033e-02,  7.9609e-02,  3.5239e-01]],\n\n         [[-6.2882e-01, -1.0027e+00, -7.9888e-01,  2.2886e-01,  9.8556e-02],\n          [-4.0301e-01,  5.7363e-02,  1.3824e-01,  1.0644e-01,  2.3789e-01],\n          [ 8.6603e-02, -3.5375e-01,  3.0650e-01,  3.1171e-01,  1.0653e+00],\n          [-2.5172e-01, -4.3575e-01, -4.6259e-01, -2.0050e-01,  8.4253e-01],\n          [-5.3329e-01, -6.4683e-01, -3.2927e-01, -2.3447e-01,  9.1453e-01]]]], requires_grad=True)\nconv2.bias \t Parameter containing:\ntensor([ 0.4177, -0.2403, -0.1402, -0.1611, -0.1229, -0.3927,  0.0597, -0.3876,  0.2276, -0.0955,  0.2956,  0.0469],\n       requires_grad=True)\nfc1.weight \t Parameter containing:\ntensor([[-0.7336,  0.2031, -0.5911,  ..., -0.0866,  0.0043, -0.3339],\n        [-1.0232,  0.2033,  0.4480,  ..., -0.2638,  0.2193,  0.3349],\n        [-0.2619, -0.0134, -0.0255,  ..., -0.3753, -0.0015, -0.2508],\n        ...,\n        [ 0.1880,  0.1172,  0.2671,  ...,  0.3841,  0.4597, -0.1349],\n        [-0.1034, -0.0491,  0.0053,  ..., -0.2486, -0.1904, -0.0219],\n        [ 0.0082, -0.3764, -0.1188,  ..., -1.0588,  0.0446,  0.0130]], requires_grad=True)\nfc1.bias \t Parameter containing:\ntensor([ 0.0380,  0.3078, -0.0781,  0.2443, -0.7777, -0.2500, -0.1030, -0.1265, -0.1841,  0.6496,  0.3628, -0.0285,\n        -0.1447,  1.3236,  0.5181, -0.1242, -0.3617,  0.7464,  0.4744,  0.1860,  0.4000, -0.1216,  0.5198,  0.2839,\n        -0.0927, -0.0656,  0.5216, -0.2990, -0.1327, -0.2166,  0.0073,  0.4987, -0.3059, -0.2728,  0.2544,  0.2004,\n         0.0832, -0.0276,  0.4343, -0.6041,  0.1756, -0.0228, -0.3266, -0.0175, -0.2215,  0.2268,  0.5098, -0.0343,\n        -0.0960,  0.4715,  0.1654, -0.0419, -0.4811, -0.3082, -0.2680, -0.1310, -0.0271, -0.1616,  0.0269,  0.1843,\n        -0.3819, -0.0788,  0.1116,  0.0978, -0.1130,  0.2393, -0.0194,  0.4723, -0.3185, -0.0685, -0.0836, -0.0344,\n        -0.0783, -0.0806, -0.0902, -0.3521, -0.4556,  0.9650,  0.0964, -0.0145, -0.0591, -0.0898, -0.0522, -0.0505,\n         0.2968, -0.2725, -0.2356, -0.2316, -0.1640, -0.0925,  0.2524, -0.0144, -0.1079,  0.3433, -0.3137,  0.5885,\n        -0.1278,  0.3158, -0.1554,  0.2031, -0.3930,  0.2727, -0.0681, -0.0349,  0.4607, -0.3826,  0.4353,  0.9321,\n         0.2524, -0.3619,  0.2178,  0.3551,  0.5974, -0.1277,  0.5359, -0.1573, -0.3255, -0.1487, -0.0311,  0.2010],\n       requires_grad=True)\nfc2.weight \t Parameter containing:\ntensor([[ 1.9393e-02, -2.2917e-01, -2.2255e-03,  ..., -2.0304e-01,  5.3482e-02, -5.8507e-02],\n        [ 6.5203e-02, -5.9199e-01,  3.8021e-02,  ..., -6.1747e-01, -1.0397e-02, -7.0940e-02],\n        [-2.7117e-01, -3.7476e-01,  2.1879e-03,  ..., -5.4487e-02, -8.3844e-02, -3.5892e-01],\n        ...,\n        [-3.4234e-02,  9.6582e-02,  6.2652e-02,  ...,  1.9377e-02,  3.5148e-02,  3.5447e-04],\n        [ 7.2972e-02, -6.0496e-01, -2.4996e-02,  ...,  2.6446e-03,  1.0942e-01, -3.0434e-01],\n        [ 7.5571e-02,  1.1624e-01, -2.9096e-02,  ...,  5.0156e-02,  3.3268e-02, -2.5585e-01]], requires_grad=True)\nfc2.bias \t Parameter containing:\ntensor([-0.6045, -0.4199, -0.2435,  0.2883, -0.4424, -0.5963, -0.3550, -0.7967,  0.1610,  0.6115,  0.2765, -0.4591,\n        -0.9608, -0.1378,  0.3573, -0.2526, -0.2263, -0.1215, -0.1101,  0.0506, -0.1150,  0.9731,  0.1724, -0.0086,\n        -0.3688,  0.3601, -0.9403, -0.1385, -0.4508,  0.4390,  0.2442, -0.3588,  0.5229, -0.6415,  0.3877, -0.5357,\n         0.1504,  1.0140, -0.1788,  1.0206,  0.7563, -0.0856,  0.6607,  0.8346,  0.1608,  0.3675, -0.2859,  0.3979,\n        -0.3892,  0.8263,  0.0721,  0.1207, -0.5577, -0.4005, -0.5275,  0.2936,  0.3453,  0.1880, -0.7702,  0.0258],\n       requires_grad=True)\nout.weight \t Parameter containing:\ntensor([[ 9.5848e-02, -3.3623e-01, -2.6587e-01,  6.1628e-02,  1.8178e-02, -1.3813e-01, -2.1888e-01, -3.0339e-01,\n         -2.4906e-01, -2.9969e-02,  4.6766e-02,  1.3264e-01, -1.4551e-02, -3.4727e-01, -2.0629e-01, -2.3633e-01,\n          2.4477e-03, -1.4297e-01,  1.2212e-01,  1.6454e-02,  9.6021e-02,  1.1009e-01,  1.0725e-01, -6.4340e-02,\n          6.4487e-02, -9.3993e-02,  1.0651e-01,  7.6635e-02,  1.5741e-01,  1.9864e-01,  6.3381e-02, -4.7006e-01,\n         -1.1338e-02, -2.2169e-01,  1.6446e-01, -3.5513e-01, -8.0054e-01,  3.8703e-01, -1.1572e-01,  2.7320e-01,\n         -2.0405e-01,  1.5223e-01,  3.5672e-02, -6.4457e-02,  1.4853e-01,  1.5425e-01, -2.4581e-02, -4.4928e-01,\n         -7.3325e-02, -7.3196e-01, -2.2467e-01, -2.1595e-01, -1.5313e-01, -5.7772e-01, -1.2461e-02, -6.7530e-01,\n          2.3075e-01, -3.5006e-01, -3.1096e-01,  1.6540e-01],\n        [ 3.3484e-01, -2.6842e-01, -4.3512e-01,  2.8873e-01, -4.4576e-01, -5.6418e-01, -5.8148e-01, -3.6052e-01,\n         -2.8115e-01, -6.5942e-02, -1.5247e-01, -5.8061e-02, -6.4785e-02, -3.7335e-01, -6.8798e-02,  1.0482e-01,\n          1.4114e-01,  2.6203e-01, -1.0709e-01, -1.6227e-01, -1.7832e-02, -3.6708e-01, -6.0324e-01,  1.1435e-02,\n          2.2264e-01,  2.1425e-01,  1.9031e-01, -1.6400e-01,  3.4192e-02, -4.7059e-01,  2.9420e-01, -8.5401e-02,\n         -7.3145e-01, -8.4400e-01, -6.7472e-02, -4.4876e-01, -8.1819e-02, -1.1183e-01, -4.0890e-01, -3.0846e-01,\n         -2.1742e-01, -3.6603e-02,  8.9861e-02, -4.8737e-01, -1.0663e-01,  2.1406e-02, -7.2668e-02, -4.5271e-01,\n          1.2192e-03, -5.8641e-01,  3.4997e-01, -1.0526e-01, -1.5931e-01, -6.2363e-01, -5.7955e-01, -5.3768e-01,\n         -4.6879e-01, -2.9327e-01, -8.9702e-01,  1.9568e-02],\n        [ 1.5203e-01, -9.1896e-01, -1.3906e-01, -2.7853e-02, -1.1099e+00, -4.1863e-01,  1.4599e-01, -5.2173e-01,\n          3.5011e-02,  2.7066e-01, -4.0508e-02, -6.9217e-02, -1.0499e+00,  2.3328e-01, -1.3334e-01,  8.4439e-02,\n         -1.8060e-01, -1.2000e-01, -4.5230e-02,  9.1384e-02, -1.3661e-01,  2.0286e-01, -1.1119e-01, -2.0499e-02,\n          2.1669e-02, -2.2126e-01, -3.1695e-01, -5.9910e-02, -7.9114e-02, -1.3263e-01,  1.5498e-01, -2.7102e-01,\n         -3.3012e-01, -8.4099e-02,  5.9504e-02,  1.9401e-01, -6.1059e-01, -3.3959e-01,  7.8036e-02,  3.0182e-01,\n          2.8437e-01, -7.0211e-03,  9.4746e-02, -3.4839e-01,  8.7645e-02,  6.5179e-02, -1.9727e-01,  2.0176e-01,\n          4.4092e-01, -5.1360e-01, -6.4295e-02, -1.3393e-01, -4.7203e-02,  1.9934e-02, -2.3898e-01, -4.8808e-01,\n          1.3851e-01, -5.0799e-01, -4.8148e-01, -2.3464e-01],\n        [-2.2025e-01,  2.3486e-01, -1.7061e-01, -1.8850e-01, -5.0548e-01,  7.7901e-02, -4.9179e-01, -7.0319e-01,\n          1.6312e-02, -7.9934e-01, -5.7039e-02, -1.8152e-01, -2.9061e-01, -4.7019e-01,  1.8299e-01,  4.8128e-02,\n         -8.5389e-03,  2.0097e-01,  5.8185e-02, -1.7561e-01,  3.4285e-01,  2.1673e-01, -7.8604e-01,  6.6500e-03,\n         -1.6240e-01,  2.5815e-01, -2.1510e-01, -2.6810e-02,  8.3251e-02, -4.3072e-01,  1.0105e-01, -5.7654e-01,\n         -5.7930e-01, -6.6092e-01,  1.1944e-01, -5.2508e-01, -1.6021e-01,  1.2386e-01,  4.5815e-04, -1.7707e-01,\n          1.6157e-01, -4.5305e-02,  2.8990e-01, -3.3939e-01,  1.1268e-01, -1.0710e-01, -1.4747e-01, -8.5340e-01,\n         -6.1601e-02, -1.2033e+00, -5.6470e-02,  8.8643e-03,  2.4592e-02, -2.2507e-01, -6.0406e-01, -8.4547e-01,\n         -2.0306e-01, -1.2392e-01, -6.2042e-01,  7.4921e-02],\n        [ 1.4214e-01, -9.9499e-01,  1.7355e-02,  1.2077e-01, -6.6390e-01, -1.8918e-01, -6.4108e-01, -4.1362e-01,\n          3.3055e-01,  2.7510e-01, -1.2705e-01, -5.9128e-02, -6.6012e-01,  3.1104e-01,  3.1503e-01, -1.6362e-01,\n         -9.7100e-02, -6.0428e-02, -9.1321e-02,  1.2867e-01, -1.6699e-01,  1.9563e-02,  3.5092e-01,  3.0819e-01,\n          6.0839e-02,  1.2858e-01,  2.7245e-01, -9.9281e-02, -9.2706e-02,  7.2850e-02,  1.4118e-02,  3.5587e-01,\n         -2.3606e-01, -2.4973e-01, -1.8351e-01,  1.8018e-01, -6.3512e-01, -2.9119e-01,  5.8631e-02, -4.9878e-02,\n          3.0180e-01, -5.2563e-02,  1.0642e-01, -5.7049e-01,  1.2641e-01, -2.5103e-01, -1.7342e-01,  1.3116e-01,\n          9.9806e-02, -5.3948e-01, -2.3875e-01,  2.3288e-02, -3.9091e-01, -1.7395e-01, -4.8663e-01, -4.8262e-01,\n          1.3052e-01, -4.9831e-01, -1.0459e+00, -1.4123e-01],\n        [-1.0442e+00, -5.7313e-01, -1.2951e+00, -3.9754e-01, -2.7685e-01,  4.0554e-02, -5.3006e-01,  5.7843e-02,\n         -5.0979e-01, -5.7991e-01,  1.1843e-01, -2.3299e-01,  6.4336e-02, -1.0593e-02, -3.9042e-01,  1.2814e-01,\n         -1.1194e-01, -2.1520e-01, -5.5486e-01, -4.9274e-02,  2.3249e-01, -5.2873e-01, -5.1239e-01, -2.1115e-03,\n         -1.9258e-01, -7.0777e-01, -4.9781e-01,  4.3128e-02, -5.3517e-02, -1.5911e-01, -1.4296e-01,  4.9362e-02,\n          1.8103e-01, -2.2746e-01,  2.2259e-01, -3.5672e-02,  1.4977e-01, -7.6418e-01, -2.6714e-01, -7.0491e-01,\n         -9.2758e-01, -3.6690e-02, -3.5639e-01,  8.6305e-02,  7.3153e-03,  1.0380e-01,  3.5377e-02,  1.7574e-01,\n         -1.2363e-01, -8.2670e-02, -1.6390e-01,  1.4470e-01, -1.2520e-01, -3.0979e-01, -6.5217e-01,  3.8049e-01,\n         -4.5285e-01,  7.7074e-03, -4.8551e-01,  1.4723e-01],\n        [ 1.4703e-01,  3.4024e-01, -2.6036e-01,  1.7396e-01, -4.5586e-01, -1.6777e-02, -2.5067e-01, -5.0787e-01,\n          2.3944e-01,  4.2520e-02, -1.2157e-01, -4.2575e-01,  1.0598e-01, -2.5785e-01, -5.9656e-02, -1.0721e-01,\n          5.1301e-03, -1.9145e-01, -3.3015e-02, -3.3280e-01,  9.6957e-02,  1.2976e-01,  5.3951e-02,  1.1575e-01,\n         -1.4185e-01,  5.6479e-02,  4.8152e-02, -8.8089e-04, -2.1296e-02,  1.8157e-01,  8.2145e-02, -3.9506e-01,\n         -4.5638e-01, -3.2524e-01, -2.3620e-01, -3.5957e-01, -4.4798e-01,  2.9932e-01,  2.5148e-02,  3.4407e-01,\n          1.9666e-01, -4.0854e-02,  6.9590e-02, -2.0504e-01,  5.7032e-02, -6.4033e-02, -2.1031e-01, -2.3599e-02,\n          1.8726e-02, -9.2943e-01,  1.2703e-01, -1.2362e-01,  6.8196e-02,  4.5282e-02,  2.2115e-01, -6.2555e-01,\n          1.5394e-01, -4.4904e-01,  2.5790e-01,  9.9797e-02],\n        [-2.5940e-01, -5.8330e-02, -1.2621e+00, -6.6348e-01,  1.8281e-01,  2.3960e-02, -6.9741e-01, -8.2065e-03,\n         -4.2545e-01, -4.1031e-01, -7.4370e-02,  1.3637e-01, -2.1810e-01,  1.7779e-01, -3.2638e-01,  2.0391e-02,\n          3.2815e-02, -2.5357e-01,  2.3011e-01, -6.9959e-02,  6.0918e-03, -1.0755e+00, -3.1664e-01, -1.0166e-01,\n          1.8261e-02, -6.3602e-01, -2.3059e-01, -6.7525e-02, -2.1664e-02, -2.6863e-01, -4.6100e-01, -3.6635e-01,\n          1.6361e-01, -4.2082e-01, -1.8102e-01, -2.1908e-01,  1.0079e-01, -1.6054e+00,  2.7717e-01, -7.9565e-01,\n         -1.2027e+00, -3.9388e-02, -2.3142e-01,  1.3923e-01, -1.3977e-01,  2.0983e-01, -1.1254e-02,  1.3054e-01,\n         -1.2733e-01,  2.3893e-01, -1.9382e-01,  5.9083e-02, -3.0674e-02, -3.4187e-01, -3.5958e-01,  1.3688e-01,\n          3.7491e-02,  2.9609e-01, -5.8390e-01, -3.5332e-01],\n        [-5.0437e-01, -9.7421e-01, -7.9366e-02,  1.6226e-01, -2.5170e-01, -7.1805e-01, -1.6660e-01, -2.2276e-01,\n          4.3992e-02,  1.3906e-02,  2.8040e-01, -2.8554e-01, -6.6086e-02, -1.1618e-01, -2.7551e-01, -1.1214e-01,\n         -3.0912e-01, -8.4089e-02, -5.5943e-02,  1.9910e-01, -3.8655e-01,  2.7980e-02, -5.4171e-01, -8.5833e-02,\n         -1.4877e-01,  1.2529e-02, -3.4919e-01, -5.4638e-02, -6.0291e-02,  2.1138e-01, -3.2174e-01, -2.3139e-01,\n         -9.0133e-02, -9.2395e-01, -1.0037e-01, -9.7770e-01, -1.9501e-01, -5.5333e-01,  4.7257e-02, -5.6008e-03,\n         -6.6106e-01,  4.6874e-02,  1.9208e-01,  3.3416e-01,  1.4290e-01,  3.1789e-01, -2.3387e-01, -2.7255e-01,\n         -2.6617e-02, -5.1966e-01,  1.6753e-01,  1.8182e-01,  8.4830e-02, -1.2034e-01, -3.2531e-02, -1.8149e-01,\n          1.9760e-01, -3.6121e-01, -5.4791e-01,  1.2269e-01],\n        [-4.5900e-01, -3.2943e-01, -1.3417e+00, -2.5969e-01, -2.7357e-01, -2.6037e-01, -7.7098e-01, -1.2495e-01,\n         -2.8713e-01, -1.5765e-01,  1.0684e-01, -2.3342e-01, -2.6481e-01, -4.7050e-02, -8.8216e-01, -2.5370e-01,\n         -2.2404e-01, -5.5723e-01,  9.4763e-02, -2.1818e-02, -3.9565e-02, -7.0772e-01, -7.4637e-01, -1.7476e-01,\n         -2.4144e-01, -4.4024e-01, -7.4787e-01,  6.7264e-02,  1.5491e-02,  1.0108e-02, -6.8196e-01,  1.2933e-01,\n          1.3476e-01,  2.7220e-02,  3.1649e-01, -1.7604e-01,  1.9163e-01, -8.9960e-01, -3.9644e-01, -8.0932e-01,\n         -7.7237e-01,  5.6375e-02, -6.9958e-01,  1.8523e-01, -1.9188e-02, -3.5703e-01,  1.7265e-01,  2.3019e-01,\n          1.6758e-02, -6.7677e-02, -4.2724e-01, -1.4007e-01, -1.7806e-02, -3.3953e-01, -8.7322e-01, -1.9338e-02,\n         -4.2331e-02,  4.2777e-01, -3.3439e-01,  7.5073e-02]], requires_grad=True)\nout.bias \t Parameter containing:\ntensor([-0.0725, -0.7327,  0.2540,  0.1153, -0.3637, -0.1213,  0.4334,  0.3600,  0.3669, -0.5698], requires_grad=True)\n"
    }
   ],
   "source": [
    "for name, param in network.named_parameters():\n",
    "    print(name, '\\t', param)"
   ]
  }
 ]
}